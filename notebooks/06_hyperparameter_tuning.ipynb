{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d342168b",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2361bcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, roc_auc_score, f1_score,\\\n",
    "precision_score, roc_auc_score, classification_report, fbeta_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13726419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>target</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "      <th>cp_4</th>\n",
       "      <th>restecg_1</th>\n",
       "      <th>restecg_2</th>\n",
       "      <th>thal_6</th>\n",
       "      <th>thal_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex  trestbps   chol  fbs  thalach  exang  oldpeak  slope   ca  \\\n",
       "0  63.0  1.0     145.0  233.0  1.0    150.0    0.0      2.3    3.0  0.0   \n",
       "1  67.0  1.0     160.0  286.0  0.0    108.0    1.0      1.5    2.0  3.0   \n",
       "\n",
       "   target  cp_2  cp_3  cp_4  restecg_1  restecg_2  thal_6  thal_7  \n",
       "0       0     0     0     0          0          1       1       0  \n",
       "1       1     0     0     1          0          1       0       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/heart_disease.csv'\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "NUMERIC_COLS = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "CATEGORICAL_COLS = [f for f in df.columns if f not in NUMERIC_COLS]\n",
    "\n",
    "# SELECTED_FEATS = ['oldpeak', 'thalach', 'ca', 'cp_4', 'exang', 'sex']\n",
    "\n",
    "# NUMERIC_COLS = list(set(NUMERIC_COLS) & set(SELECTED_FEATS))\n",
    "# CATEGORICAL_COLS = list(set(CATEGORICAL_COLS) & set(SELECTED_FEATS))\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd91000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X = df.drop(\"target\", axis=1)\n",
    "y = df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed594b0",
   "metadata": {},
   "source": [
    "# Models Pipelines And Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e2225",
   "metadata": {},
   "source": [
    "- Best three models that showed **high accuracy**:\n",
    "    - Random forest\n",
    "    - Support vector machine\n",
    "    - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6bcbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLS.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f08a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor that scales only numeric features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), NUMERIC_COLS),\n",
    "        ('cat', 'passthrough', CATEGORICAL_COLS)\n",
    "    ]\n",
    ")\n",
    "MODELS = [\"Random Forest\", \"SVM\", \"Logistic Regression\"] # referred to as 0, 1, 2 (don't change order)\n",
    "METRICS = {x:None for x in MODELS} # metrics for each model\n",
    "SCORING = make_scorer(fbeta_score, beta=2) # f2-scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469becc",
   "metadata": {},
   "source": [
    "- The **F2-score** optimally balances the clinical reality that false negatives (missed heart disease) are **more costly** than false positives in most healthcare scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c18da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline function\n",
    "def baseline_model(pipeline, model):\n",
    "    \"\"\"calcualate the baseline model scores on train data using cross validation and test data\n",
    "    and save it in metrics \"\"\"\n",
    "    # used Baseline model clone to not affect the original pipeline\n",
    "    baseline = clone(pipeline)\n",
    "\n",
    "    # Calculate the Cross-Validation Score on the training data\n",
    "    cv_scores = cross_val_score(\n",
    "        baseline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        scoring=SCORING,\n",
    "        cv=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    avg_cv_score = np.mean(cv_scores)\n",
    "    \n",
    "    # Fit the model\n",
    "    baseline.fit(X_train, y_train)\n",
    "    baseline_score = fbeta_score(y_test, baseline.predict(X_test), beta=2)\n",
    "    \n",
    "    print(f\"Baseline F2 score (CV): {avg_cv_score:.4f}\")\n",
    "    print(f\"Baseline F2 score (Test): {baseline_score:.4f}\")\n",
    "    \n",
    "    # save the scores\n",
    "    METRICS[model] = {'Baseline_CV':avg_cv_score, 'Baseline_Test':baseline_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958408df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Random Grid Search Function\n",
    "def rand_search(pipeline, params):\n",
    "    \"\"\"returns best parameters from phase 1\"\"\"\n",
    "    print(\"=== PHASE 1: RANDOM_GRIDSEARCH ===\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        params,\n",
    "        n_iter=50,\n",
    "        cv=10, \n",
    "        scoring=SCORING,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best Random Search parameters (Phase 1):\")\n",
    "    for key, value in random_search.best_params_.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best CV score:{random_search.best_score_:.3f}\")\n",
    "\n",
    "    return random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2028f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Grid Search Function\n",
    "def grid_search(pipeline, params):\n",
    "    \"\"\"returns grid search object from phase 2\"\"\"\n",
    "    print(\"=== PHASE 2: GRIDSEARCH ===\")\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        params,\n",
    "        cv=10,\n",
    "        scoring=SCORING,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best Grid Search parameters (Phase 2):\")\n",
    "    for key, value in grid_search.best_params_.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best CV Score: {grid_search.best_score_:.3f}\")\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b0e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on Test Data\n",
    "def evaluate(grid_search):\n",
    "    \"\"\"returns a dictionary of error metrics\"\"\"\n",
    "    best_est = grid_search.best_estimator_\n",
    "    y_pred = best_est.predict(X_test)\n",
    "    y_pred_proba = best_est.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    dic = {\n",
    "        'Best_Params': grid_search.best_params_,\n",
    "        'Best_CV': grid_search.best_score_,\n",
    "        'Best_Test': fbeta_score(y_test, y_pred, beta=2),\n",
    "        'Best_Test_Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Best_Test_Precision': precision_score(y_test, y_pred),\n",
    "        'Best_Test_Recall': recall_score(y_test, y_pred),\n",
    "    }\n",
    "    \n",
    "    return dic\n",
    "\n",
    "# Print the results\n",
    "def print_eval(dic):\n",
    "    \"\"\"prints evaluation results on test data\"\"\"\n",
    "    print(\"==== PERFORMANCE ====\")\n",
    "    for metric, value in dic.items():  # Skip first items\n",
    "        if metric in [\"Best_Params\"]:\n",
    "            continue\n",
    "        print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f39274",
   "metadata": {},
   "source": [
    "# Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d287b",
   "metadata": {},
   "source": [
    "#### 1. Random forest Basline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c0e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F2 score (CV): 0.7636\n",
      "Baseline F2 score (Test): 0.9441\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "#     ('preprocessor', preprocessor), # since binary trees doesn't need scaling\n",
    "    ('cls', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "# Baseline Random Forest score\n",
    "baseline_model(rf_pipeline, MODELS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4377f",
   "metadata": {},
   "source": [
    "#### 2. Random forest grid search (phase 1): \n",
    "- Using **RandomizedSearchCV** with variety of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06a3657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1: RANDOM_GRIDSEARCH ===\n",
      "Best Random Search parameters (Phase 1):\n",
      "  cls__bootstrap: False\n",
      "  cls__max_depth: 8\n",
      "  cls__max_features: log2\n",
      "  cls__min_samples_leaf: 5\n",
      "  cls__min_samples_split: 2\n",
      "  cls__n_estimators: 253\n",
      "Best CV score:0.781\n"
     ]
    }
   ],
   "source": [
    "# Random Search for Random Forest Parameter Grid\n",
    "rf_param_dist = {\n",
    "    'cls__n_estimators': randint(50, 300), # minimum number of samples at a leaf node\n",
    "    'cls__max_depth': randint(3, 15), # maximum depth of each tree.\n",
    "    'cls__min_samples_split': randint(2, 20), # minimum number of samples to split a node.\n",
    "    'cls__min_samples_leaf': randint(1, 10), # minimum number of samples at a leaf node.\n",
    "    'cls__max_features': ['sqrt', 'log2', 0.5, 0.7], # number of features considered at each split.\n",
    "    'cls__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "best = rand_search(rf_pipeline, rf_param_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a961d5",
   "metadata": {},
   "source": [
    "#### 3. Random forest grid search (phase 2):\n",
    "- Using **GridSearchCV** with more specific parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe1dc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 2: GRIDSEARCH ===\n",
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "Best Grid Search parameters (Phase 2):\n",
      "  cls__bootstrap: False\n",
      "  cls__max_depth: 7\n",
      "  cls__max_features: log2\n",
      "  cls__min_samples_leaf: 5\n",
      "  cls__min_samples_split: 2\n",
      "  cls__n_estimators: 278\n",
      "Best CV Score: 0.782\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for Random Forest Parameter Grid\n",
    "rf_param_grid = {\n",
    "    'cls__n_estimators': [int(best['cls__n_estimators'] * 0.9), best['cls__n_estimators'], int(best['cls__n_estimators'] * 1.1)],\n",
    "    'cls__max_depth': [best['cls__max_depth'] - 1, best['cls__max_depth'], best['cls__max_depth'] + 1],\n",
    "    'cls__min_samples_split': [best['cls__min_samples_split'], best['cls__min_samples_split']+1, best['cls__min_samples_split']+2],\n",
    "    'cls__min_samples_leaf': [max(1, best['cls__min_samples_leaf']-1), best['cls__min_samples_leaf'], best['cls__min_samples_leaf']+1],\n",
    "    'cls__max_features': [best['cls__max_features']],\n",
    "    'cls__bootstrap': [best['cls__bootstrap']]\n",
    "}\n",
    "rf_gs = grid_search(rf_pipeline, rf_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563bebca",
   "metadata": {},
   "source": [
    "#### 4. Evaluation for random forest best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef9faa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== PERFORMANCE ====\n",
      "Baseline_CV: 0.764\n",
      "Baseline_Test: 0.944\n",
      "Best_CV: 0.782\n",
      "Best_Test: 0.922\n",
      "Best_Test_Accuracy: 0.918\n",
      "Best_Test_Precision: 0.897\n",
      "Best_Test_Recall: 0.929\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model\n",
    "METRICS[MODELS[0]].update(**evaluate(rf_gs))\n",
    "print_eval(METRICS[MODELS[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdd54aa",
   "metadata": {},
   "source": [
    "# Support vector machine hypertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efb5193",
   "metadata": {},
   "source": [
    "#### 1. Support vector machine Basline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0db0c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F2 score (CV): 0.7461\n",
      "Baseline F2 score (Test): 0.9220\n"
     ]
    }
   ],
   "source": [
    "# SVM Pipeline\n",
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('cls', SVC(probability=True, random_state=42))  # probability=True for AUC\n",
    "])\n",
    "\n",
    "# Baseline support vector score\n",
    "baseline_model(svm_pipeline, MODELS[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd397b",
   "metadata": {},
   "source": [
    "#### 2. SVM grid search (phase 1):\n",
    "- Using **RandomizedSearchCV** with variety of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d8eb39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1: RANDOM_GRIDSEARCH ===\n",
      "Best Random Search parameters (Phase 1):\n",
      "  cls__kernel: linear\n",
      "  cls__gamma: 10.0\n",
      "  cls__class_weight: balanced\n",
      "  cls__C: 2.976351441631316\n",
      "Best CV score:0.798\n"
     ]
    }
   ],
   "source": [
    "# Random Search for Support Vector Machine Parameter Grid\n",
    "\n",
    "# linear and rbf kernel\n",
    "svm_param_dist = {\n",
    "    \"cls__C\": np.logspace(-3, 3, 20),        # 0.001 → 1000\n",
    "    \"cls__gamma\": np.logspace(-4, 1, 20),    # 0.0001 → 10\n",
    "    \"cls__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"cls__class_weight\": [None, \"balanced\"]\n",
    "    \n",
    "}\n",
    "\n",
    "# poly kernel\n",
    "# svm_param_dist = {\n",
    "#     \"cls__kernel\": [\"poly\"],\n",
    "#     \"cls__degree\": [2, 3],\n",
    "#     \"cls__C\": [0.1, 1, 10],\n",
    "#     \"cls__gamma\": [\"scale\", \"auto\", 0.01, 0.1],\n",
    "#     \"cls__coef0\": [0, 1]\n",
    "# }\n",
    "\n",
    "best = rand_search(svm_pipeline, svm_param_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9caa0",
   "metadata": {},
   "source": [
    "#### 3. SVM grid search (phase 2):\n",
    "- Using **GridSearchCV** with more specific parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99cf194b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 2: GRIDSEARCH ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "Best Grid Search parameters (Phase 2):\n",
      "  cls__C: 0.9412049672680661\n",
      "  cls__class_weight: balanced\n",
      "  cls__kernel: linear\n",
      "Best CV Score: 0.813\n"
     ]
    }
   ],
   "source": [
    "# Random Search for Support Vector Machine Parameter Grid\n",
    "\n",
    "# for linear kernel\n",
    "svm_param_grid = {\n",
    "    \"cls__C\": np.logspace(np.log10(best['cls__C']) - 0.5, np.log10(best['cls__C']) + 0.5, 10),\n",
    "    \"cls__kernel\": [best['cls__kernel']],\n",
    "    \"cls__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "svm_gs = grid_search(svm_pipeline, svm_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e16a6",
   "metadata": {},
   "source": [
    "#### 4. Evaluation for Support Vector Machine best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce7b3c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== PERFORMANCE ====\n",
      "Baseline_CV: 0.746\n",
      "Baseline_Test: 0.922\n",
      "Best_CV: 0.813\n",
      "Best_Test: 0.909\n",
      "Best_Test_Accuracy: 0.885\n",
      "Best_Test_Precision: 0.839\n",
      "Best_Test_Recall: 0.929\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model\n",
    "METRICS[MODELS[1]].update(**evaluate(svm_gs))\n",
    "print_eval(METRICS[MODELS[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1225124",
   "metadata": {},
   "source": [
    "## Logistic regression hypertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376b7667",
   "metadata": {},
   "source": [
    "#### 1. Logistic regression Basline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9874133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F2 score (CV): 0.7865\n",
      "Baseline F2 score (Test): 0.9091\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('cls', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Baseline Logistic Regression\n",
    "baseline_model(lr_pipeline, MODELS[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825a2a5",
   "metadata": {},
   "source": [
    "#### 2. Logistic Regression grid search (phase 1):\n",
    "- Using **RandomizedSearchCV** with variety of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6c2956a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 1: RANDOM_GRIDSEARCH ===\n",
      "Best Random Search parameters (Phase 1):\n",
      "  cls__solver: liblinear\n",
      "  cls__penalty: l2\n",
      "  cls__l1_ratio: 0.75\n",
      "  cls__class_weight: balanced\n",
      "  cls__C: 0.0071968567300115215\n",
      "Best CV score:0.825\n"
     ]
    }
   ],
   "source": [
    "# Random Search for Logistic Regression Parameter Grid\n",
    "\n",
    "lr_param_dist = {\n",
    "    \"cls__penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"cls__C\": np.logspace(-3, 3, 50),    # 0.001 → 1000\n",
    "    \"cls__solver\": [\"liblinear\", \"saga\"],  # supports l1 & elasticnet\n",
    "    \"cls__class_weight\": [None, \"balanced\", [{0: 1, 1: W} for W in [1.5, 3, 5, 10]]],\n",
    "    \"cls__l1_ratio\": np.linspace(0, 1, 5)  # only matters if elasticnet\n",
    "}\n",
    "\n",
    "best = rand_search(lr_pipeline, lr_param_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034391d",
   "metadata": {},
   "source": [
    "#### 3. Logistic Regression grid search (phase 2):\n",
    "- Using **GridSearchCV** with more specific parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4230a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PHASE 2: GRIDSEARCH ===\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Best Grid Search parameters (Phase 2):\n",
      "  cls__C: 0.00046415888336127773\n",
      "  cls__class_weight: balanced\n",
      "  cls__penalty: l2\n",
      "  cls__solver: liblinear\n",
      "Best CV Score: 0.858\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for Logistic Regression Parameter Grid\n",
    "lr_param_grid = {\n",
    "    \"cls__C\": np.logspace(-4, -1, 10),   # narrower range\n",
    "    \"cls__penalty\": [best[\"cls__penalty\"]],\n",
    "    \"cls__solver\": [best[\"cls__solver\"]],\n",
    "    \"cls__class_weight\": [best[\"cls__class_weight\"]]\n",
    "}\n",
    "\n",
    "lr_gs = grid_search(lr_pipeline, lr_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccb025",
   "metadata": {},
   "source": [
    "### 4. Evaluation for Logistic Regression best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a623b918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== PERFORMANCE ====\n",
      "Baseline_CV: 0.787\n",
      "Baseline_Test: 0.909\n",
      "Best_CV: 0.858\n",
      "Best_Test: 0.886\n",
      "Best_Test_Accuracy: 0.705\n",
      "Best_Test_Precision: 0.609\n",
      "Best_Test_Recall: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model\n",
    "METRICS[MODELS[2]].update(**evaluate(lr_gs))\n",
    "print_eval(METRICS[MODELS[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e636bd99",
   "metadata": {},
   "source": [
    "# Comapring Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6d90e2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL RESULTS COMPARISON\n",
      "================================================================================\n",
      "                 Model  Baseline_CV   Best_CV  Baseline_Test  Best_Test  \\\n",
      "0        Random Forest     0.763605  0.782450       0.944056   0.921986   \n",
      "1                  SVM     0.746063  0.812989       0.921986   0.909091   \n",
      "2  Logistic Regression     0.786503  0.858348       0.909091   0.886076   \n",
      "\n",
      "   Best_Test_Accuracy  Best_Test_Precision  Best_Test_Recall  \n",
      "0            0.918033             0.896552          0.928571  \n",
      "1            0.885246             0.838710          0.928571  \n",
      "2            0.704918             0.608696          1.000000  \n",
      "\n",
      "================================================================================\n",
      "BEST PARAMETERS FOR EACH MODEL\n",
      "================================================================================\n",
      "\n",
      "Random Forest:\n",
      "  cls__bootstrap: False\n",
      "  cls__max_depth: 7\n",
      "  cls__max_features: log2\n",
      "  cls__min_samples_leaf: 5\n",
      "  cls__min_samples_split: 2\n",
      "  cls__n_estimators: 278\n",
      "\n",
      "SVM:\n",
      "  cls__C: 0.9412049672680661\n",
      "  cls__class_weight: balanced\n",
      "  cls__kernel: linear\n",
      "\n",
      "Logistic Regression:\n",
      "  cls__C: 0.00046415888336127773\n",
      "  cls__class_weight: balanced\n",
      "  cls__penalty: l2\n",
      "  cls__solver: liblinear\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "models = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Random Forest\",\n",
    "    \"SVM\",\n",
    "]\n",
    "all_grids = {\n",
    "        'Random Forest': rf_gs,\n",
    "        'SVM': svm_gs,\n",
    "        'Logistic Regression': lr_gs\n",
    "}\n",
    "# Save all Results in a data frame\n",
    "results = []\n",
    "for name in MODELS:\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        **METRICS[name],\n",
    "    })\n",
    "df_results = pd.DataFrame(results).sort_values(by='Best_Test', ascending=False)\n",
    "cols_order = ['Best_Params', 'Model', 'Baseline_CV', 'Best_CV', 'Baseline_Test', 'Best_Test',\n",
    "             'Best_Test_Accuracy', 'Best_Test_Precision', 'Best_Test_Recall']\n",
    "df_results = df_results[cols_order]\n",
    "\n",
    "# Print Results\n",
    "print(df_results.drop(columns='Best_Params'))\n",
    "\n",
    "# Display best parameters for each model\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BEST PARAMETERS FOR EACH MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for i in results:\n",
    "    print(f\"\\n{i['Model']}:\")\n",
    "    for param, value in i['Best_Params'].items():\n",
    "        print(f\"  {param}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd2e5a",
   "metadata": {},
   "source": [
    "- While the **Logistic Regression** achieved a perfect $1.0$ Recall (it identified all positive cases), its Precision is very low ($0.608696$). This low Precision means it had many false positives, which significantly dragged down its overall $F_2$ score to $0.886076$.\n",
    "- The **Random Forest** achieved a high Recall ($0.928571$) while maintaining a strong Precision ($0.896552$), leading to the superior final $F_2$ score\n",
    "- So we can say that **Random Forest** is the most effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e7cbe1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/final_model.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_gs.best_estimator_, \"../models/final_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01f12f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
